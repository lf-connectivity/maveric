{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "sys.path.append(f\"{Path().absolute().parent}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "customInput": null,
        "customOutput": null,
        "executionStartTime": 1664303063351,
        "executionStopTime": 1664303063392,
        "originalKey": "da3d7012-72dc-4cb6-8e01-09ed85b7bc4d",
        "requestMsgId": "da3d7012-72dc-4cb6-8e01-09ed85b7bc4d",
        "showInput": true
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "# (c) Facebook, Inc. and its affiliates. Confidential and proprietary.\n",
        "\n",
        "import logging\n",
        "import os\n",
        "from enum import Enum\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "\n",
        "from radp.digital_twin.utils.gis_tools import GISTools\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "import gpytorch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from radp.digital_twin.utils.constants import (\n",
        "    ANTENNA_GAIN,\n",
        "    CELL_CARRIER_FREQ_MHZ,\n",
        "    CELL_EL_DEG,\n",
        "    CELL_ID,\n",
        "    CELL_LAT,\n",
        "    CELL_LON,\n",
        "    CELL_RXPWR_DBM,\n",
        "    HRX,\n",
        "    HTX,\n",
        "    LOG_DISTANCE,\n",
        "    RELATIVE_BEARING,\n",
        "    RXPOWER_DBM,\n",
        "    RXPOWER_STDDEV_DBM,\n",
        "    SIM_IDX,\n",
        ")\n",
        "\n",
        "\n",
        "class NormMethod(Enum):\n",
        "    MINMAX = \"minmax\"  # {value - min}/{max - min}\n",
        "    ZSCORE = \"zscore\"  # {value - mean}/{std}\n",
        "\n",
        "\n",
        "class ExactGPModel(gpytorch.models.ExactGP):\n",
        "    # We will use the simplest form of GP model, exact inference\n",
        "    def __init__(self, train_x, train_y, likelihood):\n",
        "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
        "        self.mean_module = gpytorch.means.ConstantMean(batch_shape=torch.Size([1]))\n",
        "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
        "            gpytorch.kernels.RBFKernel(batch_shape=torch.Size([1])),\n",
        "            batch_shape=torch.Size([1]),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean_x = self.mean_module(x)\n",
        "        covar_x = self.covar_module(x)\n",
        "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
        "\n",
        "\n",
        "class BayesianDigitalTwin:\n",
        "    def __init__(\n",
        "        self,\n",
        "        data_in: List[pd.DataFrame],\n",
        "        x_columns: List[str],\n",
        "        y_columns: List[str],\n",
        "        norm_method: NormMethod = NormMethod.MINMAX,\n",
        "        x_max: Optional[Dict[str, float]] = None,\n",
        "        x_min: Optional[Dict[str, float]] = None,\n",
        "    ):\n",
        "        \"\"\"\n",
        "\n",
        "        `data_in` is a list of Pandas dataframes, where each one corresponds to\n",
        "        training data for one cell. `stats` is a list of Pandas dataframes, of the\n",
        "        same length as `data_in`, where each contains statistics of the corresponding\n",
        "        cell, to be used for pre-training normalization and post-prediction de-normalization.\n",
        "\n",
        "        `x_columns` specifies the columns to train on, and `y_columns` specifies the columns\n",
        "        to predict. These must be present in `data_in`.\n",
        "\n",
        "        `x_max` and `x_min` contains optional user-specified max and min values for columns\n",
        "        in `data_in` -- if these are provided, they are used instead of observed ranges\n",
        "        during input pre-training normalization.\n",
        "\n",
        "        `data_in` and `stats` may be constructed using `get_percell_data`.\n",
        "        \"\"\"\n",
        "        self.is_cuda = False\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.set_device(0)\n",
        "            self.is_cuda = True\n",
        "\n",
        "        self.cell_ids = [data_in_cell.cell_id.unique()[0] for data_in_cell in data_in]\n",
        "        self.num_cells = len(self.cell_ids)\n",
        "        self.x_columns = x_columns\n",
        "        self.y_columns = y_columns\n",
        "        self.num_features = len(self.x_columns)\n",
        "        self.n_train = data_in[0].shape[0]\n",
        "\n",
        "        self.cell_stats = [cell_data.describe() for cell_data in data_in]\n",
        "        self.xmeans = []\n",
        "        self.xstds = []\n",
        "        self.xmax = []\n",
        "        self.xmin = []\n",
        "        self.ymeans = []\n",
        "        self.ystds = []\n",
        "\n",
        "        self.norm_method = norm_method\n",
        "\n",
        "        # Create Gaussian Process Regression (GPR) model; independent outputs.\n",
        "        # produce normalization ranges\n",
        "        for m in range(self.num_cells):\n",
        "            self.xmax.append(self.cell_stats[m].loc[\"max\", self.x_columns])\n",
        "            self.xmin.append(self.cell_stats[m].loc[\"min\", self.x_columns])\n",
        "            # if explicitly provided, replace and use instead of empirical ranges\n",
        "            if x_max:\n",
        "                for k, v in x_max.items():\n",
        "                    if k in self.x_columns:\n",
        "                        self.xmax[m][k] = v\n",
        "            if x_min:\n",
        "                for k, v in x_min.items():\n",
        "                    if k in self.x_columns:\n",
        "                        self.xmin[m][k] = v\n",
        "            self.xmeans.append(self.cell_stats[m].loc[\"mean\", self.x_columns])\n",
        "            self.xstds.append(self.cell_stats[m].loc[\"std\", self.x_columns])\n",
        "            self.ymeans.append(self.cell_stats[m].loc[\"mean\", self.y_columns])\n",
        "            self.ystds.append(self.cell_stats[m].loc[\"std\", self.y_columns])\n",
        "\n",
        "        # create training tensors\n",
        "\n",
        "        #train_X, train_Y = self._create_training_tensors(self)\n",
        "\n",
        "        train_X = torch.zeros(\n",
        "            [self.num_cells, self.n_train, self.num_features], dtype=torch.float32\n",
        "        )\n",
        "        train_Y = torch.zeros([self.num_cells, self.n_train], dtype=torch.float32)\n",
        "\n",
        "        for m in range(self.num_cells):\n",
        "            if self.norm_method == NormMethod.MINMAX:\n",
        "                train_x_cell = (data_in[m][self.x_columns] - self.xmin[m]) / (\n",
        "                    self.xmax[m] - self.xmin[m]\n",
        "                )\n",
        "            elif self.norm_method == NormMethod.ZSCORE:\n",
        "                train_x_cell = (\n",
        "                    data_in[m][self.x_columns] - self.xmeans[m]\n",
        "                ) / self.xstds[m]\n",
        "\n",
        "            train_X_cell = torch.tensor(\n",
        "                train_x_cell.iloc[:, :].values, dtype=torch.float32\n",
        "            )\n",
        "\n",
        "            train_y_cell = (data_in[m][self.y_columns] - self.ymeans[m]) / self.ystds[m]\n",
        "            train_Y_cell = torch.tensor(\n",
        "                train_y_cell.iloc[:, :].values, dtype=torch.float32\n",
        "            )\n",
        "\n",
        "            train_X[m] = train_X_cell.reshape(shape=(1, -1, self.num_features))\n",
        "            train_Y[m] = torch.transpose(train_Y_cell, 0, 1)\n",
        "\n",
        "        # initialize likelihood and model\n",
        "        likelihood = gpytorch.likelihoods.GaussianLikelihood(\n",
        "            batch_shape=torch.Size([self.num_cells])\n",
        "        )\n",
        "\n",
        "        self.model = ExactGPModel(train_X, train_Y, likelihood)\n",
        "\n",
        "    @staticmethod\n",
        "    def split_training_and_test_data(\n",
        "        data_in: pd.DataFrame,\n",
        "        n_sim: int,\n",
        "        alpha: float,\n",
        "    ) -> (List[pd.DataFrame], List[pd.DataFrame]):\n",
        "        \"\"\"Split the simulation data groups into test and training sets.\n",
        "\n",
        "        data_in: aggregated simulation data in the form of a dataframe (test+train)\n",
        "        n_sim: number of simulations aggregated in data_in\n",
        "        alpha: percent of simulation runs used for training\n",
        "\n",
        "        Each row of `data_in` corresponds to one pixel, and the columns\n",
        "        are assumed to contain :\n",
        "            - settings corresponding to one or more cells, with column names\n",
        "            `settng_name_<n>` for different settings of interest and where\n",
        "            n refers to the index of a cell in the modeled cluster.\n",
        "            - `rx_loc1` and `rx_loc2`, the geo-coordinates for the pixel\n",
        "            - `rxpower_dbm_<n>`, the received powers for the cell with index n\n",
        "            - `rsrp_dbm` is the max power and `cell_id` is the cell index of that cell\n",
        "            - `sim_idx` is the simulation index\n",
        "\n",
        "        Example:\n",
        "        ['cell_azimuth_deg_1', 'cell_azimuth_deg_2', 'cell_azimuth_deg_3',\n",
        "        'cell_elec_tilt_deg_1', 'cell_elec_tilt_deg_2', 'cell_elec_tilt_deg_3',\n",
        "        'cell_mech_tilt_deg_1', 'cell_mech_tilt_deg_2', 'cell_mech_tilt_deg_3',\n",
        "        'cell_txpower_dbm_1', 'cell_txpower_dbm_2', 'cell_txpower_dbm_3',\n",
        "        'rxpower_dbm_1', 'rxpower_dbm_2', 'rxpower_dbm_3', 'rsrp_dbm',\n",
        "        'sinr_db', 'cell_id', 'rx_loc1', 'rx_loc2', 'sim_idx']\n",
        "\n",
        "        \"\"\"\n",
        "        n_training_group = np.max([int(alpha * 0.01 * n_sim), 1])\n",
        "        n_test_group = n_sim - n_training_group\n",
        "        logging.info(\n",
        "            f\"Splitting data into {n_training_group} training and {n_test_group} test groups...\"\n",
        "        )\n",
        "        training_data = data_in[data_in[SIM_IDX] > n_test_group].reset_index(drop=True)\n",
        "        test_data = data_in[data_in[SIM_IDX] <= n_test_group].reset_index(drop=True)\n",
        "        stats = data_in.describe(include=\"all\")\n",
        "        return training_data, test_data, stats, n_training_group\n",
        "\n",
        "    @staticmethod\n",
        "    def create_prediction_frames(\n",
        "        site_config_df: pd.DataFrame,\n",
        "        prediction_frame_template: pd.DataFrame,\n",
        "    ) -> Dict[str, pd.DataFrame]:\n",
        "        \"\"\"\n",
        "        `site_config_df` : 1 unique cell per row, contains at least the columns\n",
        "            [cell_lat, cell_lon, cell_el_deg, cell_az_deg, cell_id]\n",
        "            Assumption : `bayesian_digital_twin` was trained with respect tp `site_config_df`\n",
        "        `prediction_frame_template` : 1 prediction point per row, contains columsn [loc_x, loc_y]\n",
        "            e.g. loc_x is longitude, and loc_y is latitude\n",
        "        \"\"\"\n",
        "\n",
        "        prediction_dfs: Dict[str, pd.DataFrame] = {}\n",
        "\n",
        "        for c in site_config_df.itertuples():\n",
        "\n",
        "            prediction_df = prediction_frame_template.copy()\n",
        "\n",
        "            prediction_df[CELL_LAT] = c.cell_lat\n",
        "            prediction_df[CELL_LON] = c.cell_lon\n",
        "            prediction_df[CELL_EL_DEG] = c.cell_el_deg\n",
        "            prediction_df[CELL_ID] = c.cell_id\n",
        "            prediction_df[CELL_CARRIER_FREQ_MHZ] = c.cell_carrier_freq_mhz\n",
        "            prediction_df[HTX] = c.hTx\n",
        "            prediction_df[HRX] = c.hRx\n",
        "\n",
        "            prediction_df[LOG_DISTANCE] = [\n",
        "                GISTools.get_log_distance(\n",
        "                    c.cell_lat,\n",
        "                    c.cell_lon,\n",
        "                    lat,\n",
        "                    lon,\n",
        "                )\n",
        "                for lat, lon in zip(\n",
        "                    prediction_frame_template.loc_y, prediction_frame_template.loc_x\n",
        "                )\n",
        "            ]\n",
        "\n",
        "            prediction_df[RELATIVE_BEARING] = [\n",
        "                GISTools.get_relative_bearing(\n",
        "                    c.cell_az_deg,\n",
        "                    c.cell_lat,\n",
        "                    c.cell_lon,\n",
        "                    lat,\n",
        "                    lon,\n",
        "                )\n",
        "                for lat, lon in zip(\n",
        "                    prediction_frame_template.loc_y, prediction_frame_template.loc_x\n",
        "                )\n",
        "            ]\n",
        "\n",
        "            prediction_df[ANTENNA_GAIN] = GISTools.get_antenna_gain(\n",
        "                c.hTx, c.hRx, prediction_df[LOG_DISTANCE], c.cell_el_deg\n",
        "            )\n",
        "\n",
        "            prediction_dfs[c.cell_id] = prediction_df\n",
        "        return prediction_dfs\n",
        "\n",
        "    @staticmethod\n",
        "    def get_percell_data(\n",
        "        data_in: pd.DataFrame,\n",
        "        all_idxs: List[int],\n",
        "        desired_idxs: List[int],\n",
        "        n_samples: int,\n",
        "        sample_cells_independently: bool = False,\n",
        "        choose_strongest_samples_percell: bool = False,\n",
        "        invalid_value: float = -500.0,\n",
        "        seed: int = 0,\n",
        "    ) -> (List[pd.DataFrame], List[pd.DataFrame]):\n",
        "        \"\"\"Split training data at cell level & compute per-cell statistics.\n",
        "\n",
        "        data_in: training data across all cells and for one or more simulations\n",
        "        desired_idxs: integer list of cell indexes to get data on\n",
        "        n_samples: number of random samples per cell\n",
        "        sample_cells_independently: True, if lat/lon to be sampled independently for each cell\n",
        "        choose_strongest_samples_percell: if `sample_cells_independently` is set to True,\n",
        "            this parameter governs whether cells are sampled randomly (if set to False),\n",
        "            or if cells are sampled at points where they likely attached (if set to True)\n",
        "\n",
        "        Each row of `data_in` corresponds to one pixel, and the columns\n",
        "        are assumed to contain :\n",
        "            - settings corresponding to one or more cells, with column names\n",
        "            `settng_name_<n>` for different settings of interest and where\n",
        "            n refers to the id of a cell in the modeled cluster.\n",
        "            - `rx_loc1` and `rx_loc2`, the geo-coordinates for the pixel\n",
        "            - `rxpower_dbm_<n>`, the received powers for the cell with id n\n",
        "            - `rsrp_dbm` is the max power and `cell_id` is the cell index of that cell\n",
        "            - `sim_idx` is the simulation index\n",
        "        Example:\n",
        "        ['cell_azimuth_deg_1', 'cell_azimuth_deg_2', 'cell_azimuth_deg_3',\n",
        "        'cell_elec_tilt_deg_1', 'cell_elec_tilt_deg_2', 'cell_elec_tilt_deg_3',\n",
        "        'cell_mech_tilt_deg_1', 'cell_mech_tilt_deg_2', 'cell_mech_tilt_deg_3',\n",
        "        'cell_txpower_dbm_1', 'cell_txpower_dbm_2', 'cell_txpower_dbm_3',\n",
        "        'rxpower_dbm_1', 'rxpower_dbm_2', 'rxpower_dbm_3', 'rsrp_dbm',\n",
        "        'sinr_db', 'cell_id', 'rx_loc1', 'rx_loc2', 'sim_idx']\n",
        "\n",
        "        The output contains a two lists, corresponding in order\n",
        "        to the indices in `desired_idxs` :\n",
        "            1. per cell data\n",
        "            2. per cell statistics\n",
        "        Each element of the per cell data list contains the following columns:\n",
        "            - settings corresponding to given cell, with column names\n",
        "            `settng_name` for different settings of interest\n",
        "            - `rx_loc1` and `rx_loc2`, the geo-coordinates for the pixel\n",
        "            - `rxpower_dbm`, the received power for the given cell\n",
        "            - `rxpower_dbm_<n>`, the received powers for the cell with id n (other than given)\n",
        "            - `rsrp_dbm` is the max power and `cell_id` is the cell id of that cell\n",
        "            - `sim_idx` is the simulation index\n",
        "        Example:\n",
        "        ['cell_azimuth_deg', 'cell_elec_tilt_deg', 'cell_mech_tilt_deg',\n",
        "        'cell_txpower_dbm', 'rxpower_dbm_1', 'rxpower_dbm_2', 'rxpower_dbm',\n",
        "        'rsrp_dbm', 'sinr_db', 'cell_id', 'rx_loc1', 'rx_loc2', 'group',\n",
        "        'sim_idx']\n",
        "        \"\"\"\n",
        "        data_out = []\n",
        "        stats = []\n",
        "\n",
        "        data_in_sampled = data_in\n",
        "\n",
        "        if not sample_cells_independently:\n",
        "            # find pixels where all cells have valid values\n",
        "            data_in_valid = data_in[\n",
        "                data_in[data_in == invalid_value].count(axis=1) == 0\n",
        "            ]\n",
        "            # sample before splitting\n",
        "            data_in_sampled = data_in_valid.sample(\n",
        "                n=min(n_samples, len(data_in_valid)), random_state=(seed)\n",
        "            )\n",
        "\n",
        "        for m in desired_idxs:\n",
        "            to_strip = []\n",
        "            for n in all_idxs:\n",
        "                # drop data from other cells\n",
        "                if n != m:\n",
        "                    to_strip.extend(list(data_in_sampled.filter(regex=f\"cell_.+_{n}$\")))\n",
        "            data_cell = data_in_sampled.drop(\n",
        "                to_strip,\n",
        "                axis=1,\n",
        "            )\n",
        "            data_cell.columns = [\n",
        "                col.replace(f\"_{m}\", \"\") if col.endswith(f\"_{m}\") else col\n",
        "                for col in data_cell.columns\n",
        "            ]\n",
        "\n",
        "            data_cell_sampled = data_cell\n",
        "\n",
        "            if sample_cells_independently:\n",
        "                # filter out invalid values\n",
        "                data_cell_valid = data_cell[data_cell.cell_rxpwr_dbm != invalid_value]\n",
        "                if choose_strongest_samples_percell:\n",
        "                    data_cell_sampled = data_cell_valid.sort_values(\n",
        "                        CELL_RXPWR_DBM, ascending=False\n",
        "                    ).head(n=min(n_samples, len(data_cell_valid)))\n",
        "                else:\n",
        "                    # get n_samples independent random samples inside training groups\n",
        "                    data_cell_sampled = data_cell_valid.sample(\n",
        "                        n=min(n_samples, len(data_cell_valid)), random_state=(seed + m)\n",
        "                    )\n",
        "\n",
        "            data_out.append(data_cell_sampled.reset_index(drop=True))\n",
        "\n",
        "            stats_cell = data_cell_sampled.describe(include=\"all\")\n",
        "            stats.append(stats_cell)\n",
        "\n",
        "        return data_out, stats\n",
        "\n",
        "    def train_distributed_gpmodel(\n",
        "        self,\n",
        "        maxiter: int = 100,\n",
        "        lr: float = 0.05,\n",
        "        stopping_threshold: float = 1e-4,\n",
        "        load_model: bool = False,\n",
        "        save_model: bool = False,\n",
        "        model_path: Optional[str] = None,\n",
        "        model_name: Optional[str] = None,\n",
        "    ) -> List[float]:\n",
        "        loss_vs_iter = np.zeros(maxiter)\n",
        "        # Train model\n",
        "        if load_model:\n",
        "            logging.info(\"Now loading GP model (this should be quick...)\")\n",
        "            state_dict = torch.load(model_path + model_name)\n",
        "            self.model.load_state_dict(state_dict)\n",
        "            if self.is_cuda:\n",
        "                logging.info(\"Cuda enabled for model.\")\n",
        "                model = self.model.cuda()\n",
        "        else:\n",
        "            self.model.train()\n",
        "            # \"Loss\" for GPs - the marginal log likelihood\n",
        "            mll = gpytorch.mlls.ExactMarginalLogLikelihood(\n",
        "                self.model.likelihood, self.model\n",
        "            )\n",
        "            optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
        "\n",
        "            train_X, train_Y = (\n",
        "                mll.model.train_inputs,\n",
        "                mll.model.train_targets,\n",
        "            )\n",
        "\n",
        "            if self.is_cuda:\n",
        "                logging.info(torch.__config__.show().split(\"\\n\"))\n",
        "                logging.info(\"Cuda enabled for training data.\")\n",
        "                train_X = train_X[0].cuda()\n",
        "                train_Y = train_Y.cuda()\n",
        "                model = model.cuda()\n",
        "                mll = mll.cuda()\n",
        "\n",
        "            last_loss = float(\"-inf\")\n",
        "            for i in range(maxiter):\n",
        "                optimizer.zero_grad()\n",
        "                output = self.model(*train_X)\n",
        "                loss = -mll(output, train_Y).sum()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                this_loss = loss.item()\n",
        "                loss_vs_iter[i] = this_loss\n",
        "                delta = this_loss - last_loss\n",
        "                last_loss = this_loss\n",
        "                logging.info(\n",
        "                    \"Iter %d/%d - Loss: %.3f (delta=%.6f)\"\n",
        "                    % (i + 1, maxiter, this_loss, delta)\n",
        "                )\n",
        "                if abs(delta) < stopping_threshold:\n",
        "                    logging.info(\"Stopping criteria met...exiting.\")\n",
        "                    break\n",
        "\n",
        "        # Save model\n",
        "        if save_model:\n",
        "            if not os.path.exists(model_path):\n",
        "                os.makedirs(model_path)\n",
        "            torch.save(self.model.state_dict(), model_path + model_name)\n",
        "            logging.info(f\"Saved trained model to: {model_path + model_name}\")\n",
        "        return loss_vs_iter\n",
        "\n",
        "    \n",
        "    # def _create_training_tensors(\n",
        "    #     self,\n",
        "    #     n_train,\n",
        "    #     ):\n",
        "\n",
        "    #     train_X = torch.zeros(\n",
        "    #         [self.num_cells, n_train, self.num_features], dtype=torch.float32\n",
        "    #     )\n",
        "        \n",
        "    #     train_Y = torch.zeros([self.num_cells, n_train], dtype=torch.float32)\n",
        "        \n",
        "\n",
        "    #     for m in range(self.num_cells):\n",
        "    #         if self.norm_method == NormMethod.MINMAX:\n",
        "    #             train_x_cell = (data_in[m][self.x_columns] - self.xmin[m]) / (\n",
        "    #                 self.xmax[m] - self.xmin[m]\n",
        "    #             )\n",
        "    #         elif self.norm_method == NormMethod.ZSCORE:\n",
        "    #             train_x_cell = (\n",
        "    #                 data_in[m][self.x_columns] - self.xmeans[m]\n",
        "    #             ) / self.xstds[m]\n",
        "\n",
        "    #         train_X_cell = torch.tensor(\n",
        "    #             train_x_cell.iloc[:, :].values, dtype=torch.float32\n",
        "    #         )\n",
        "\n",
        "    #         train_y_cell = (data_in[m][self.y_columns] - self.ymeans[m]) / self.ystds[m]\n",
        "    #         train_Y_cell = torch.tensor(\n",
        "    #             train_y_cell.iloc[:, :].values, dtype=torch.float32\n",
        "    #         )\n",
        "\n",
        "    #         train_X[m] = train_X_cell.reshape(shape=(1, -1, self.num_features))\n",
        "    #         train_Y[m] = torch.transpose(train_Y_cell, 0, 1)\n",
        "\n",
        "    #         return train_X, train_Y\n",
        "\n",
        "\n",
        "    def update_model(\n",
        "        self,\n",
        "        data_in: pd.DataFrame,\n",
        "        # load_model: bool = False,\n",
        "        # save_model: bool = False,\n",
        "        # model_path: Optional[str] = None,\n",
        "        # model_name: Optional[str] = None,\n",
        "        ):\n",
        "        \"\"\"\n",
        "        Assumes that the model is already trained and prediction is run on the model at least once\n",
        "        \"\"\"\n",
        "\n",
        "        # if load_model:\n",
        "        #     logging.info(\"Now loading GP model (this should be quick...)\")\n",
        "        #     state_dict = torch.load(model_path + model_name)\n",
        "        #     self.model.load_state_dict(state_dict)\n",
        "\n",
        "        n_train = data_in[0].shape[0]\n",
        "\n",
        "        # Get train_X and train_Y, create training tensors\n",
        "        #train_X, train_Y = self._create_training_tensors(self, n_train)\n",
        "\n",
        "        train_X = torch.zeros(\n",
        "            [self.num_cells, n_train, self.num_features], dtype=torch.float32\n",
        "        )\n",
        "        \n",
        "        train_Y = torch.zeros([self.num_cells, n_train], dtype=torch.float32)\n",
        "        \n",
        "\n",
        "        for m in range(self.num_cells):\n",
        "            if self.norm_method == NormMethod.MINMAX:\n",
        "                train_x_cell = (data_in[m][self.x_columns] - self.xmin[m]) / (\n",
        "                    self.xmax[m] - self.xmin[m]\n",
        "                )\n",
        "            elif self.norm_method == NormMethod.ZSCORE:\n",
        "                train_x_cell = (\n",
        "                    data_in[m][self.x_columns] - self.xmeans[m]\n",
        "                ) / self.xstds[m]\n",
        "\n",
        "            train_X_cell = torch.tensor(\n",
        "                train_x_cell.iloc[:, :].values, dtype=torch.float32\n",
        "            )\n",
        "\n",
        "            train_y_cell = (data_in[m][self.y_columns] - self.ymeans[m]) / self.ystds[m]\n",
        "            train_Y_cell = torch.tensor(\n",
        "                train_y_cell.iloc[:, :].values, dtype=torch.float32\n",
        "            )\n",
        "\n",
        "            train_X[m] = train_X_cell.reshape(shape=(1, -1, self.num_features))\n",
        "            train_Y[m] = torch.transpose(train_Y_cell, 0, 1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # updated_train_X = torch.cat((self.train_X, train_X))\n",
        "        # updated_train_Y = torch.cat((self.train_Y, train_Y))\n",
        "\n",
        "        # logging.info(\"Now loading GP model (this should be quick...)\")\n",
        "        # state_dict = torch.load(model_path + model_name)\n",
        "        # self.model.load_state_dict(state_dict)\n",
        "        #print(\"update train_x\", train_X)\n",
        "        #print(\"update train_y\", train_Y)\n",
        "\n",
        "        #print(\"concat train_X\", updated_train_X)\n",
        "        #print(\"concat train_Y\", updated_train_Y)\n",
        "        \n",
        "        #self.model.set_train_data(inputs=updated_train_X, targets=updated_train_Y, strict=False)\n",
        "        self.model = self.model.get_fantasy_model(inputs=train_X, targets=train_Y)\n",
        "\n",
        "        # if save_model:\n",
        "        #     if not os.path.exists(model_path):\n",
        "        #         os.makedirs(model_path)\n",
        "        #     torch.save(self.model.state_dict(), model_path + model_name)\n",
        "        #     logging.info(f\"Saved trained model to: {model_path + model_name}\")\n",
        "\n",
        "\n",
        "    def predict_distributed_gpmodel(\n",
        "        self,\n",
        "        prediction_dfs: List[pd.DataFrame],\n",
        "        # load_model: bool = False,\n",
        "        # save_model: bool = False,\n",
        "        # model_path: Optional[str] = None,\n",
        "        # model_name: Optional[str] = None,\n",
        "    ) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        \"\"\"Predicts Rx power, RSRP and SINR.\n",
        "\n",
        "        `prediction_dfs` : one prediction dataframe, per cell.\n",
        "\n",
        "        It is assumed that columns loc_x and loc_y in each dataframe inside `prediction_dfs`\n",
        "        are the same, and that they appear in the same order.\n",
        "\n",
        "        Returns the prediction mean and standard deviation for Rx power\n",
        "        as numpy ndarrays, for all locations in the dataframe,\n",
        "        in the same order given, with one such array per cell.\n",
        "\n",
        "        Mutates `prediction_dfs` and adds columns for predicted Rx mean and stddev.\n",
        "\n",
        "        Returns, in order :\n",
        "            prediction mean for Rx power (one numpy ndarray per cell)\n",
        "            prediction std dev for Rx power (one numpy ndarray per cell)\n",
        "            combined RF dataframe with RSRP and SINR\n",
        "        \"\"\"\n",
        "\n",
        "        # if load_model:\n",
        "        #     logging.info(\"Now loading trained GP model (this should be quick...)\")\n",
        "        #     state_dict = torch.load(model_path + model_name)\n",
        "        #     self.model.load_state_dict(state_dict)\n",
        "\n",
        "        \n",
        "        self.model.eval()\n",
        "\n",
        "        num_locations = prediction_dfs[0].shape[0]\n",
        "        pred_means = torch.zeros([num_locations, self.num_cells], dtype=torch.float32)\n",
        "        pred_stds = torch.zeros([num_locations, self.num_cells], dtype=torch.float32)\n",
        "        predict_X = torch.zeros(\n",
        "            [self.num_cells, num_locations, self.num_features], dtype=torch.float32\n",
        "        )\n",
        "\n",
        "        for m in range(self.num_cells):\n",
        "            if self.norm_method == NormMethod.MINMAX:\n",
        "                predict_x_cell = (prediction_dfs[m][self.x_columns] - self.xmin[m]) / (\n",
        "                    self.xmax[m] - self.xmin[m]\n",
        "                )\n",
        "            elif self.norm_method == NormMethod.ZSCORE:\n",
        "                predict_x_cell = (\n",
        "                    prediction_dfs[m][self.x_columns] - self.xmeans[m]\n",
        "                ) / self.xstds[m]\n",
        "\n",
        "            predict_X_cell = torch.tensor(\n",
        "                predict_x_cell.iloc[:, :].values, dtype=torch.float32\n",
        "            )\n",
        "            predict_X[m] = predict_X_cell.reshape(shape=(1, -1, self.num_features))\n",
        "\n",
        "        if self.is_cuda:\n",
        "            logging.info(\"Cuda enabled for test data.\")\n",
        "            predict_X = predict_X.cuda()\n",
        "\n",
        "        with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
        "            observed_pred = self.model.likelihood(self.model(predict_X))\n",
        "            mean = observed_pred.mean\n",
        "            var = observed_pred.variance\n",
        "\n",
        "        pred_means = mean.detach().cpu().numpy() * self.ystds + self.ymeans\n",
        "        pred_stds = np.sqrt(var.detach().cpu().numpy()) * self.ystds\n",
        "\n",
        "        # add pred_means and pred_std to prediction_dfs\n",
        "        for idx in range(len(prediction_dfs)):\n",
        "            prediction_dfs[idx][RXPOWER_DBM] = pred_means[idx]\n",
        "            prediction_dfs[idx][RXPOWER_STDDEV_DBM] = pred_stds[idx]\n",
        "\n",
        "        # if save_model:\n",
        "        #     if not os.path.exists(model_path):\n",
        "        #         os.makedirs(model_path)\n",
        "        #     torch.save(self.model.state_dict(), model_path + model_name)\n",
        "        #     logging.info(f\"Saved predicted model to: {model_path + model_name}\")\n",
        "\n",
        "        return pred_means, pred_stds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "code_folding": [],
        "collapsed": false,
        "customInput": null,
        "customOutput": null,
        "executionStartTime": 1664303067410,
        "executionStopTime": 1664303067670,
        "hidden_ranges": [],
        "originalKey": "468216c0-d21b-4b5c-9189-9d6602935d0d",
        "requestMsgId": "468216c0-d21b-4b5c-9189-9d6602935d0d",
        "showInput": true
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "# (c) Meta Platforms, Inc. and affiliates. Confidential and proprietary.\n",
        "\n",
        "# os.system(\"persistent-storage define fbc_maveric --bucket fbc_maveric\")\n",
        "# os.system(\"persistent-storage mount --auto\")\n",
        "# buck build //bento/kernels:bento_kernel_maveric\n",
        "\n",
        "import copy\n",
        "import logging\n",
        "import os\n",
        "import random\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "# from advanced_network_planning.common.fblearner_srtm.constants import AntennaType\n",
        "from radp.digital_twin.anp.anp_simulator import AnpEngine\n",
        "from radp.digital_twin.anp.utils import plot_helper, rf_dict_to_anp_sites\n",
        "# from radp.energy_savings.energy_savings_gym import EnergySavingsGym\n",
        "# from radp.energy_savings.energy_savings_gym_dgpco import DgpcoEnergySavingsGym\n",
        "from radp.utils.gis_tools import GISTools\n",
        "# from IPython.display import HTML, display\n",
        "\n",
        "\n",
        "def seed_everything(seed: int):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "\n",
        "def anp_rf_sim(rf_dict, n_cell=1, n_sim=10):\n",
        "\n",
        "    # trim rf_dict to n_cell cells\n",
        "    rf_dict = {k: v[:n_cell] for k, v in rf_dict.items()}\n",
        "    anp_engine = AnpEngine()\n",
        "    data_boundary = None\n",
        "\n",
        "    # Init\n",
        "    sites = rf_dict_to_anp_sites(rf_dict)\n",
        "    rf_dataframe = anp_engine.sites_to_df(\n",
        "        sites,\n",
        "        rf_dict,\n",
        "        sim_idx=0,\n",
        "        data_boundary=data_boundary,\n",
        "    )\n",
        "\n",
        "    # Init: save RF data to file\n",
        "    sim_idx = 0\n",
        "    sim_idx_folder = str(sim_idx).zfill(3)\n",
        "    save_path = f\"/{BUCKET_PATH}/{SIM_DATA_PATH}/sim_{sim_idx_folder}\"\n",
        "    if not os.path.exists(save_path):\n",
        "        os.makedirs(save_path)\n",
        "    rf_dataframe.to_csv(f\"{save_path}/full_data.csv\", index=False)\n",
        "\n",
        "    # Init: save site config to file\n",
        "    site_config_df = pd.DataFrame.from_dict(rf_dict)\n",
        "    site_config_df = site_config_df.rename(\n",
        "        columns={\n",
        "            \"az_boresight_angle\": \"cell_az_deg\",\n",
        "            \"el_boresight_angle\": \"cell_el_deg\",\n",
        "        }\n",
        "    )\n",
        "    site_config_df = site_config_df[[\"cell_lat\", \"cell_lon\", \"cell_az_deg\", \"cell_el_deg\", \"cell_id\", \"hTx\", \"hRx\"]]\n",
        "    import itertools\n",
        "\n",
        "    site_config_df[\"cell_id\"] = list(itertools.chain(*site_config_df[\"cell_id\"].tolist()))\n",
        "    site_config_df[\"nRx\"] = rf_dataframe.groupby(\"cell_id\").size().tolist()\n",
        "    site_config_df.to_csv(f\"{save_path}/site_config.csv\", index=False)\n",
        "\n",
        "    # Init: plot RF data\n",
        "    plt = plot_helper(\n",
        "        lat=rf_dataframe.loc_y,\n",
        "        lon=rf_dataframe.loc_x,\n",
        "        data=rf_dataframe.rsrp_dbm,\n",
        "        sites=sites,\n",
        "        title=f\"AnpEngine() | {len(rf_dataframe.index):,} points | cell_el_deg = [{', '.join(f'{x:.1f}' for x in site_config_df.cell_el_deg.tolist())}]\",\n",
        "        label=\"RSRP (dBm)\",\n",
        "        colormap=\"twilight\",\n",
        "        markersize=25,\n",
        "        hist_flag=True,\n",
        "    )\n",
        "    plt.show()\n",
        "\n",
        "    # Loop\n",
        "    rf_dict_random = copy.deepcopy(rf_dict)\n",
        "\n",
        "    for sim_idx in range(1, n_sim + 1):\n",
        "        print(f\"Now simulating: {sim_idx} of {n_sim}...\")\n",
        "        tilt_random = random.choices(tilt_set, k=n_cell)\n",
        "        rf_dict_random[\"el_boresight_angle\"] = tilt_random\n",
        "        sites = rf_dict_to_anp_sites(rf_dict_random)\n",
        "        rf_dataframe = anp_engine.sites_to_df(\n",
        "            sites,\n",
        "            rf_dict_random,\n",
        "            sim_idx=sim_idx,\n",
        "            data_boundary=data_boundary,\n",
        "        )\n",
        "\n",
        "        # Save RF data to file\n",
        "        sim_idx_folder = str(sim_idx).zfill(3)\n",
        "        save_path = f\"/{BUCKET_PATH}/{SIM_DATA_PATH}/sim_{sim_idx_folder}\"\n",
        "        if not os.path.exists(save_path):\n",
        "            os.makedirs(save_path)\n",
        "        rf_dataframe.to_csv(f\"{save_path}/full_data.csv\", index=False)\n",
        "\n",
        "        # Save site config to file\n",
        "        site_config_df = pd.DataFrame.from_dict(rf_dict_random)\n",
        "        site_config_df = site_config_df.rename(\n",
        "            columns={\n",
        "                \"az_boresight_angle\": \"cell_az_deg\",\n",
        "                \"el_boresight_angle\": \"cell_el_deg\",\n",
        "            }\n",
        "        )\n",
        "        site_config_df = site_config_df[\n",
        "            [\n",
        "                \"cell_lat\",\n",
        "                \"cell_lon\",\n",
        "                \"cell_az_deg\",\n",
        "                \"cell_el_deg\",\n",
        "                \"cell_id\",\n",
        "                \"hTx\",\n",
        "                \"hRx\",\n",
        "            ]\n",
        "        ]\n",
        "\n",
        "        site_config_df[\"cell_id\"] = list(itertools.chain(*site_config_df[\"cell_id\"].tolist()))\n",
        "        site_config_df[\"nRx\"] = rf_dataframe.groupby(\"cell_id\").size().tolist()\n",
        "        site_config_df.to_csv(f\"{save_path}/site_config.csv\", index=False)\n",
        "\n",
        "        # Plot RF data\n",
        "        plt = plot_helper(\n",
        "            lat=rf_dataframe.loc_y,\n",
        "            lon=rf_dataframe.loc_x,\n",
        "            data=rf_dataframe.rsrp_dbm,\n",
        "            sites=sites,\n",
        "            title=f\"AnpEngine() | {len(rf_dataframe.index):,} points | cell_el_deg = [{', '.join(f'{x:.1f}' for x in site_config_df.cell_el_deg.tolist())}]\",\n",
        "            label=\"RSRP (dBm)\",\n",
        "            colormap=\"twilight\",\n",
        "            markersize=25,\n",
        "            hist_flag=True,\n",
        "        )\n",
        "        plt.show()\n",
        "    return\n",
        "\n",
        "def get_training_and_test_data(\n",
        "    desired_idxs_train: List[int],\n",
        "    desired_idxs_test: List[int],\n",
        "    p_train=20,\n",
        "    p_test=100,\n",
        "    n_sim=10,\n",
        "):\n",
        "    site_config_path = \"sim_\" + str(0).zfill(3) + \"/site_config.csv\"\n",
        "    site_config_df = pd.read_csv(f\"/{BUCKET_PATH}/{SIM_DATA_PATH}/{site_config_path}\")\n",
        "\n",
        "    nRx = site_config_df[\"nRx\"].sum()\n",
        "    n_sample_train = round(p_train * 0.01 * nRx)\n",
        "    n_sample_test = round(p_test * 0.01 * nRx)\n",
        "    n_cell = len(site_config_df.index)\n",
        "    logging.info(\"==========\")\n",
        "    logging.info(f\"nRx={nRx}, n_sample_train={n_sample_train}, n_sample_test={n_sample_test}\")\n",
        "    logging.info(\"==========\")\n",
        "\n",
        "    metadata_df = pd.DataFrame({\"cell_id\": [1, 2, 3], \"idx\": [1, 2, 3]})\n",
        "    idx_cell_id_mapping = dict(zip(metadata_df.idx, metadata_df.cell_id))\n",
        "    #print(idx_cell_id_mapping)\n",
        "    # desired_idxs = desired_idxs\n",
        "    # print(desired_idxs)\n",
        "    # test\n",
        "    test_sim_idx = n_sim\n",
        "    sim_idx_folder = \"sim_\" + str(test_sim_idx).zfill(3) + \"/full_data.csv\"\n",
        "    tilt_test_df = pd.read_csv(f\"/{BUCKET_PATH}/{SIM_DATA_PATH}/{sim_idx_folder}\")\n",
        "\n",
        "    (tilt_test_per_cell_df, tilt_test_per_cell_stats,) = BayesianDigitalTwin.get_percell_data(\n",
        "        data_in=tilt_test_df,\n",
        "        all_idxs=list(idx_cell_id_mapping.keys()),\n",
        "        desired_idxs=desired_idxs_test,\n",
        "        sample_cells_independently=False,\n",
        "        n_samples=n_sample_test,\n",
        "    )\n",
        "    test_data = {}\n",
        "    for i in range(len(desired_idxs_test)):\n",
        "        test_data[desired_idxs_test[i]] = pd.concat(\n",
        "            [\n",
        "                tilt_test_per_cell_df[i],\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    # train\n",
        "    n_sim_train = n_sim - 1\n",
        "    percell_data_list = []\n",
        "    for s in range(n_sim_train):\n",
        "        sim_idx_folder = \"sim_\" + str(s + 1).zfill(3) + \"/full_data.csv\"\n",
        "        # tilt_df = pd.read_csv(os.path.join(BUCKET_PATH, SIM_DATA_PATH, sim_idx_folder))\n",
        "        tilt_df = pd.read_csv(f\"/{BUCKET_PATH}/{SIM_DATA_PATH}/{sim_idx_folder}\")\n",
        "        tilt_per_cell_df, _ = BayesianDigitalTwin.get_percell_data(\n",
        "            data_in=tilt_df,\n",
        "            all_idxs=list(idx_cell_id_mapping.keys()),\n",
        "            desired_idxs=desired_idxs_train,\n",
        "            sample_cells_independently=False,\n",
        "            n_samples=n_sample_train,\n",
        "        )\n",
        "        percell_data_list.append(tilt_per_cell_df)\n",
        "    training_data = {}\n",
        "    for i in range(len(desired_idxs_train)):\n",
        "        training_data[desired_idxs_train[i]] = pd.concat(\n",
        "            [tilt_per_cell_df[i] for tilt_per_cell_df in percell_data_list]\n",
        "        )\n",
        "\n",
        "    # train\n",
        "    for idx, training_data_idx in training_data.items():\n",
        "        train_cell_id = idx_cell_id_mapping[idx]\n",
        "        training_data_idx[\"cell_id\"] = 1\n",
        "\n",
        "        training_data_idx[\"cell_lat\"] = site_config_df[site_config_df[\"cell_id\"] == train_cell_id][\"cell_lat\"].values[0]\n",
        "\n",
        "        training_data_idx[\"cell_lon\"] = site_config_df[site_config_df[\"cell_id\"] == train_cell_id][\"cell_lon\"].values[0]\n",
        "\n",
        "        training_data_idx[\"hTx\"] = site_config_df[site_config_df[\"cell_id\"] == train_cell_id][\"hTx\"].values[0]\n",
        "\n",
        "        training_data_idx[\"hRx\"] = site_config_df[site_config_df[\"cell_id\"] == train_cell_id][\"hRx\"].values[0]\n",
        "\n",
        "        training_data_idx[\"cell_carrier_freq_mhz\"] = 1200\n",
        "\n",
        "        training_data_idx[\"log_distance\"] = [\n",
        "            GISTools.get_log_distance(\n",
        "                training_data_idx[\"cell_lat\"].values[0],\n",
        "                training_data_idx[\"cell_lon\"].values[0],\n",
        "                lat,\n",
        "                lon,\n",
        "            )\n",
        "            for lat, lon in zip(training_data_idx.loc_y, training_data_idx.loc_x)\n",
        "        ]\n",
        "\n",
        "        training_data_idx[\"relative_bearing\"] = [\n",
        "            GISTools.get_relative_bearing(\n",
        "                training_data_idx[\"cell_az_deg\"].values[0],\n",
        "                training_data_idx[\"cell_lat\"].values[0],\n",
        "                training_data_idx[\"cell_lon\"].values[0],\n",
        "                lat,\n",
        "                lon,\n",
        "            )\n",
        "            for lat, lon in zip(training_data_idx.loc_y, training_data_idx.loc_x)\n",
        "        ]\n",
        "\n",
        "        training_data_idx[\"antenna_gain\"] = GISTools.get_antenna_gain(\n",
        "            training_data_idx[\"hTx\"].values[0],\n",
        "            training_data_idx[\"hRx\"].values[0],\n",
        "            training_data_idx[\"log_distance\"],\n",
        "            training_data_idx[\"cell_el_deg\"],\n",
        "        )\n",
        "\n",
        "    # test\n",
        "    for idx, test_data_idx in test_data.items():\n",
        "        test_cell_id = idx_cell_id_mapping[idx]\n",
        "        #print(test_cell_id)\n",
        "        test_data_idx[\"cell_id\"] = 1\n",
        "        test_data_idx[\"cell_lat\"] = site_config_df[site_config_df[\"cell_id\"] == test_cell_id][\"cell_lat\"].values[0]\n",
        "        test_data_idx[\"cell_lon\"] = site_config_df[site_config_df[\"cell_id\"] == test_cell_id][\"cell_lon\"].values[0]\n",
        "        test_data_idx[\"hTx\"] = site_config_df[site_config_df[\"cell_id\"] == test_cell_id][\"hTx\"].values[0]\n",
        "        test_data_idx[\"hRx\"] = site_config_df[site_config_df[\"cell_id\"] == test_cell_id][\"hRx\"].values[0]\n",
        "\n",
        "        test_data_idx[\"cell_carrier_freq_mhz\"] = 1200\n",
        "\n",
        "        test_data_idx[\"log_distance\"] = [\n",
        "            GISTools.get_log_distance(\n",
        "                test_data_idx[\"cell_lat\"].values[0],\n",
        "                test_data_idx[\"cell_lon\"].values[0],\n",
        "                lat,\n",
        "                lon,\n",
        "            )\n",
        "            for lat, lon in zip(test_data_idx.loc_y, test_data_idx.loc_x)\n",
        "        ]\n",
        "\n",
        "        test_data_idx[\"relative_bearing\"] = [\n",
        "            GISTools.get_relative_bearing(\n",
        "                test_data_idx[\"cell_az_deg\"].values[0],\n",
        "                test_data_idx[\"cell_lat\"].values[0],\n",
        "                test_data_idx[\"cell_lon\"].values[0],\n",
        "                lat,\n",
        "                lon,\n",
        "            )\n",
        "            for lat, lon in zip(test_data_idx.loc_y, test_data_idx.loc_x)\n",
        "        ]\n",
        "\n",
        "        test_data_idx[\"antenna_gain\"] = GISTools.get_antenna_gain(\n",
        "            test_data_idx[\"hTx\"].values[0],\n",
        "            test_data_idx[\"hRx\"].values[0],\n",
        "            test_data_idx[\"log_distance\"],\n",
        "            test_data_idx[\"cell_el_deg\"],\n",
        "        )\n",
        "\n",
        "\n",
        "        test_data_final = {}\n",
        "        for idx, data in test_data.items():\n",
        "            data.sort_values(by=['loc_x', 'loc_y'],  inplace=True)\n",
        "            append_value(test_data_final, 1, data)\n",
        "\n",
        "        training_data_final = {}\n",
        "        for idx, data in training_data.items():\n",
        "            data.sort_values(by=['loc_x', 'loc_y'],  inplace=True)\n",
        "            append_value(training_data_final, 1, data)\n",
        "\n",
        "        # for cell_id, data in training_data_final.items():\n",
        "        #     print(data)\n",
        "\n",
        "    return training_data_final, test_data_final\n",
        "\n",
        "def append_value(dict_obj, key, value):\n",
        "    # Check if key exist in dict or not\n",
        "    if key in dict_obj:\n",
        "        # Key exist in dict.\n",
        "        # Check if type of value of key is list or not\n",
        "        if not isinstance(dict_obj[key], list):\n",
        "            # If type is not list then make it list\n",
        "            dict_obj[key] = [dict_obj[key]]\n",
        "        # Append the value in list\n",
        "        dict_obj[key].append(value)\n",
        "    else:\n",
        "        # As key is not in dict,\n",
        "        # so, add key-value pair\n",
        "        dict_obj[key] = value\n",
        "\n",
        "\n",
        "def bdt(\n",
        "    p_train=20,\n",
        "    p_test=100,\n",
        "    maxiter=20,\n",
        "    n_sim=10,\n",
        "    desired_idxs_train=[1],\n",
        "    desired_idxs_test=[1, 2, 3],\n",
        "    desired_idxs_train_update=[1,2],\n",
        "    desired_idxs_test_update=[1,2],\n",
        "    load_model=False,\n",
        "    save_model=False,\n",
        "    model_path=\"\",\n",
        "    model_name=\"\",\n",
        "):\n",
        "\n",
        "    ###########TRAIN AND TEST DATA##################\n",
        "    training_data, test_data = get_training_and_test_data(\n",
        "        desired_idxs_train,\n",
        "        desired_idxs_test,\n",
        "        p_train=20,\n",
        "        p_test=100,\n",
        "        n_sim=10,\n",
        "    )\n",
        "\n",
        "    training_data = {key: val[:4000] for key, val in training_data.items()}\n",
        "\n",
        "    #############TRAIN MODEL########################\n",
        "    bayesian_digital_twin_map = {}\n",
        "    for cell_id, training_data in training_data.items():\n",
        "\n",
        "        bayesian_digital_twin_map[cell_id] = BayesianDigitalTwin(\n",
        "            data_in=[training_data],\n",
        "            x_columns=[\"log_distance\", \"relative_bearing\", \"antenna_gain\"],\n",
        "            y_columns=[\"cell_rxpwr_dbm\"],\n",
        "            # norm_method=NormMethod.MINMAX,\n",
        "            x_max=None,\n",
        "            x_min=None,\n",
        "        )\n",
        "\n",
        "        loss_vs_iter = bayesian_digital_twin_map[cell_id].train_distributed_gpmodel(\n",
        "            maxiter=maxiter,\n",
        "            load_model=load_model,\n",
        "            save_model=save_model,\n",
        "            model_path=model_path,\n",
        "            model_name=model_name,\n",
        "        )\n",
        "\n",
        "        logging.info(\n",
        "            f\"\\nTrained {len(loss_vs_iter)} epochs of Bayesian Digital Twin (Gaussian Process Regression) \"\n",
        "            f\"on {len(training_data)} data points\"\n",
        "            f\" with min learning loss {min(loss_vs_iter):0.5f}, \"\n",
        "            f\"avg learning loss {np.mean(loss_vs_iter):0.5f} and final learning loss {loss_vs_iter[-1]:0.5f}\"\n",
        "        )\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.style.use(\"bmh\")\n",
        "    plt.plot(np.arange(maxiter), loss_vs_iter)\n",
        "    plt.xlabel(\"iter\")\n",
        "    plt.ylabel(\"loss\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    # evaluate over test set:\n",
        "\n",
        "    # Hack\n",
        "    # for idx, data in test_data.values():\n",
        "    #     frames = [idx, data]\n",
        "    #     result = pd.concat(frames)\n",
        "\n",
        "    # cell_initial_test_map = {k: v for k, v in result.groupby(\"cell_id\")}\n",
        "    # test_data = cell_initial_test_map\n",
        "    # = {key: val[:2000] for key, val in test_data.items()}\n",
        "\n",
        "    for cell_id, testing_data in test_data.items():\n",
        "        #prediction_dfs = list(test_data.values())\n",
        "        (pred_means, _) = bayesian_digital_twin_map[\n",
        "                cell_id\n",
        "            ].predict_distributed_gpmodel(\n",
        "                prediction_dfs=[testing_data],\n",
        "                # load_model=True,\n",
        "                # save_model=True,\n",
        "                # model_path=model_path,\n",
        "                # model_name=model_name,\n",
        "                )\n",
        "        #print(pred_means)\n",
        "        f, axs = plt.subplots(1, 2, figsize=(12, 12))\n",
        "        lons = test_data[1][\"loc_x\"].values\n",
        "        lats = test_data[1][\"loc_y\"].values\n",
        "\n",
        "        # compute RSRP as maximum over predicted rx powers\n",
        "        pred_rsrp = np.amax(pred_means, axis=0)\n",
        "        # extract true (actual) RSRP from test set\n",
        "        true_rsrp = np.maximum.reduce([test_data_cell.cell_rxpwr_dbm for test_data_cell in test_data.values()])\n",
        "        # mean absolute error\n",
        "        MAE = abs(true_rsrp - pred_rsrp).mean()\n",
        "        # mean square error\n",
        "        MSE = (abs(true_rsrp - pred_rsrp) ** 2).mean()\n",
        "        # mean absolute percentage error\n",
        "        MAPE = 100 * abs((true_rsrp - pred_rsrp) / true_rsrp).mean()\n",
        "        logging.info(\"==========\")\n",
        "        logging.info(f\"MSE = {MSE:0.5f}, MAE = {MAE:0.5f} dB, MAPE = {MAPE:0.5f} %\")\n",
        "        logging.info(\"==========\")\n",
        "        #print(\"true_rsrp\", true_rsrp)\n",
        "        axs[0].scatter(lons, lats, c=true_rsrp, cmap=\"twilight\", s=10)\n",
        "        axs[0].set_title(\n",
        "            r\"Actual RSRP\",\n",
        "            fontsize=14,\n",
        "        )\n",
        "        axs[0].set_aspect(\"equal\", \"box\")\n",
        "        axs[0].set_xticks([])\n",
        "        axs[0].set_yticks([])\n",
        "        #print(pred_rsrp)\n",
        "        axs[1].scatter(lons, lats, c=pred_rsrp, cmap=\"twilight\", s=10)\n",
        "        axs[1].set_title(\n",
        "            f\"Predicted RSRP | MAE = {MAE:0.1f} dB\",\n",
        "            fontsize=14,\n",
        "        )\n",
        "        axs[1].set_aspect(\"equal\", \"box\")\n",
        "        axs[1].set_xticks([])\n",
        "        axs[1].set_yticks([])\n",
        "\n",
        "        plt.subplots_adjust(left=0.1, bottom=0.1, right=0.9, top=0.9, wspace=0.0, hspace=0.1)\n",
        "        plt.show()\n",
        "\n",
        "    ################# UPDATE #########################\n",
        "\n",
        "    training_update_data, test_update_data = get_training_and_test_data(\n",
        "        desired_idxs_train_update,\n",
        "        desired_idxs_test_update,\n",
        "        p_train=20,\n",
        "        p_test=100,\n",
        "        n_sim=10,\n",
        "    )\n",
        "    \n",
        "    training_update_data = {key: val[4000:] for key, val in training_update_data.items()}\n",
        "    for cell_id, training_data in training_update_data.items():\n",
        "\n",
        "        bayesian_digital_twin_map[\n",
        "                cell_id\n",
        "            ].update_model(\n",
        "            data_in=list(training_update_data.values()),\n",
        "            # load_model=True,\n",
        "            # save_model=True,\n",
        "            # model_path=model_path,\n",
        "            # model_name=model_name,\n",
        "        )\n",
        "\n",
        "    # evaluate over test set:\n",
        "\n",
        "    # Hack\n",
        "    # for idx, data in test_update_data.values():\n",
        "    #     frames = [idx, data]\n",
        "    #     result = pd.concat(frames)\n",
        "\n",
        "    # cell_id_testing_data_map = {k: v for k, v in result.groupby(\"cell_id\")}\n",
        "    cell_id_testing_data_map = test_update_data\n",
        "\n",
        "    #cell_id_testing_data_map = {key: val[2000:] for key, val in test_update_data.items()}\n",
        "    for cell_id, testing_data in cell_id_testing_data_map.items():\n",
        "        prediction_dfs = list(test_update_data.values())\n",
        "        (pred_means1, _) = bayesian_digital_twin_map[\n",
        "                cell_id\n",
        "            ].predict_distributed_gpmodel(\n",
        "                prediction_dfs=[testing_data],\n",
        "                # load_model=True,\n",
        "                # save_model=True,\n",
        "                # model_path=model_path,\n",
        "                # model_name=model_name,\n",
        "                )\n",
        "        #print(pred_means1)\n",
        "        f, axs = plt.subplots(1, 2, figsize=(12, 12))\n",
        "        lons = cell_id_testing_data_map[1][\"loc_x\"].values\n",
        "        lats = cell_id_testing_data_map[1][\"loc_y\"].values\n",
        "\n",
        "        # compute RSRP as maximum over predicted rx powers\n",
        "        pred_rsrp1 = np.amax(pred_means1, axis=0)\n",
        "        # extract true (actual) RSRP from test set\n",
        "        true_rsrp1 = np.maximum.reduce([test_data_cell.cell_rxpwr_dbm for test_data_cell in cell_id_testing_data_map.values()])\n",
        "        # mean absolute error\n",
        "        MAE1 = abs(true_rsrp1 - pred_rsrp1).mean()\n",
        "        # mean square error\n",
        "        MSE1 = (abs(true_rsrp1 - pred_rsrp1) ** 2).mean()\n",
        "        # mean absolute percentage error\n",
        "        MAPE1 = 100 * abs((true_rsrp1 - pred_rsrp1) / true_rsrp1).mean()\n",
        "        logging.info(\"==========\")\n",
        "        logging.info(f\"MSE = {MSE1:0.5f}, MAE = {MAE1:0.5f} dB, MAPE = {MAPE1:0.5f} %\")\n",
        "        logging.info(\"==========\")\n",
        "\n",
        "        axs[0].scatter(lons, lats, c=true_rsrp1, cmap=\"twilight\", s=10)\n",
        "        axs[0].set_title(\n",
        "            r\"Actual RSRP\",\n",
        "            fontsize=14,\n",
        "        )\n",
        "        axs[0].set_aspect(\"equal\", \"box\")\n",
        "        axs[0].set_xticks([])\n",
        "        axs[0].set_yticks([])\n",
        "        #print(pred_rsrp1)\n",
        "        axs[1].scatter(lons, lats, c=pred_rsrp1, cmap=\"twilight\", s=10)\n",
        "        axs[1].set_title(\n",
        "            f\"Predicted RSRP | MAE = {MAE1:0.1f} dB\",\n",
        "            fontsize=14,\n",
        "        )\n",
        "        axs[1].set_aspect(\"equal\", \"box\")\n",
        "        axs[1].set_xticks([])\n",
        "        axs[1].set_yticks([])\n",
        "\n",
        "        plt.subplots_adjust(left=0.1, bottom=0.1, right=0.9, top=0.9, wspace=0.0, hspace=0.1)\n",
        "        plt.show()\n",
        "\n",
        "    return bayesian_digital_twin_map, test_data, loss_vs_iter\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "customInput": null,
        "originalKey": "b989253d-d3cf-46f9-b5ab-a128bc3d87a4",
        "showInput": false
      },
      "source": [
        "# RF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "code_folding": [],
        "collapsed": false,
        "customInput": null,
        "customOutput": null,
        "executionStartTime": 1664303069557,
        "executionStopTime": 1664303071503,
        "hidden_ranges": [],
        "originalKey": "886117e4-00f8-46b7-889a-c67d27e968f7",
        "requestMsgId": "886117e4-00f8-46b7-889a-c67d27e968f7",
        "showInput": true
      },
      "outputs": [],
      "source": [
        "n_cell = 3\n",
        "n_sim = 10\n",
        "seed = 100\n",
        "seed_everything(seed=seed)\n",
        "\n",
        "os.system(\"persistent-storage mount --auto\")\n",
        "BUCKET_PATH = \"home/sandeeprajan/rf_sim/fbc_maveric/data/digital_twin\"\n",
        "#BUCKET_PATH = \"tmp\"\n",
        "SIM_DATA_PATH = \"sim_data/simple3cell\"\n",
        "\n",
        "tilt_min, tilt_max = 10, 15\n",
        "tilt_set = list(np.arange(tilt_min, tilt_max + 1))\n",
        "\n",
        "rf_dict = {\n",
        "    \"cell_name\": [\"11\", \"12\", \"13\"],\n",
        "    \"cell_lat\": [\n",
        "        34.0000,\n",
        "        34.0000,\n",
        "        34.0000,\n",
        "    ],\n",
        "    \"cell_lon\": [\n",
        "        -102.0000,\n",
        "        -102.0000,\n",
        "        -102.0000,\n",
        "    ],\n",
        "    \"cell_id\": [[1], [2], [3]],\n",
        "    \"enb_tx_power\": [23, 23, 23],\n",
        "    \"enb_noise\": [5.0, 5.0, 5.0],\n",
        "    \"enb_antenna_gain\": [6, 6, 6],\n",
        "    \"enb_tx_diversity_gain\": [0.0, 0.0, 0.0],\n",
        "    \"enb_rx_diversity_gain\": [0.0, 0.0, 0.0],\n",
        "    \"enb_tx_misc_loss\": [0.0, 0.0, 0.0],\n",
        "    \"enb_rx_misc_loss\": [0.0, 0.0, 0.0],\n",
        "    \"az_boresight_angle\": [0, 120, 240],\n",
        "    \"el_boresight_angle\": [tilt_set[0], tilt_set[0], tilt_set[0]],\n",
        "    \"az_beamwidth\": [60, 60, 60],\n",
        "    \"el_beamwidth\": [10, 10, 10],\n",
        "    \"load_factor\": [1.0, 1.0, 1.0],\n",
        "    # \"antenna_type\": [AntennaType.triGPP, AntennaType.triGPP, AntennaType.triGPP],\n",
        "    \"max_range\": [1.0, 1.0, 1.0],\n",
        "    \"hTx\": [35, 35, 35],\n",
        "    \"hRx\": [2, 2, 2],\n",
        "    \"freq_MHz\": [1800, 1800, 1800],\n",
        "    \"penetration_loss_db\": [0.0, 0.0, 0.0],\n",
        "    \"df_loss_factor\": [0.0, 0.0, 0.0],\n",
        "    \"pl_model\": [3, 3, 3],\n",
        "    \"use_clutter\": [False, False, False],\n",
        "    \"urban_mode\": [False, False, False],\n",
        "    \"use_openmp\": [False, False, False],\n",
        "}\n",
        "\n",
        "# rf_dict = {\n",
        "#     \"cell_name\": [\"11\"],\n",
        "#     \"cell_lat\": [\n",
        "#         34.0000,\n",
        "#     ],\n",
        "#     \"cell_lon\": [\n",
        "#         -102.0000,\n",
        "#     ],\n",
        "#     \"cell_id\": [[1]],\n",
        "#     \"enb_tx_power\": [23],\n",
        "#     \"enb_noise\": [5.0],\n",
        "#     \"enb_antenna_gain\": [6],\n",
        "#     \"enb_tx_diversity_gain\": [0.0],\n",
        "#     \"enb_rx_diversity_gain\": [0.0],\n",
        "#     \"enb_tx_misc_loss\": [0.0],\n",
        "#     \"enb_rx_misc_loss\": [0.0],\n",
        "#     \"az_boresight_angle\": [0],\n",
        "#     \"el_boresight_angle\": [tilt_set[0]],\n",
        "#     \"az_beamwidth\": [60],\n",
        "#     \"el_beamwidth\": [10],\n",
        "#     \"load_factor\": [1.0],\n",
        "#     \"antenna_type\": [AntennaType.triGPP],\n",
        "#     \"max_range\": [1.0],\n",
        "#     \"hTx\": [35],\n",
        "#     \"hRx\": [2],\n",
        "#     \"freq_MHz\": [1800],\n",
        "#     \"penetration_loss_db\": [0.0],\n",
        "#     \"df_loss_factor\": [0.0],\n",
        "#     \"pl_model\": [3],\n",
        "#     \"use_clutter\": [False],\n",
        "#     \"urban_mode\": [False],\n",
        "#     \"use_openmp\": [False],\n",
        "# }\n",
        "\n",
        "#anp_rf_sim(rf_dict=rf_dict, n_cell=n_cell, n_sim=n_sim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "code_folding": [],
        "customInput": null,
        "hidden_ranges": [],
        "originalKey": "582dc381-5724-4701-8832-cd312af89a80",
        "showInput": false
      },
      "source": [
        "# Bayesian digital twin training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "customInput": null,
        "customOutput": null,
        "executionStartTime": 1663023313633,
        "executionStopTime": 1663023314603,
        "originalKey": "635eb278-1bcf-4da2-a2d8-08fbe139ada0",
        "requestMsgId": "635eb278-1bcf-4da2-a2d8-08fbe139ada0",
        "showInput": true
      },
      "outputs": [],
      "source": [
        "training_data, test_data = get_training_and_test_data(\n",
        "    desired_idxs_train = [1],\n",
        "    desired_idxs_test = [1],\n",
        "    p_train=20,\n",
        "    p_test=100,\n",
        "    n_sim=10,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "customInput": null,
        "customOutput": null,
        "executionStartTime": 1663023328920,
        "executionStopTime": 1663023329146,
        "originalKey": "20a3e73d-3d41-47e7-9464-880c0631063b",
        "requestMsgId": "20a3e73d-3d41-47e7-9464-880c0631063b",
        "showInput": true
      },
      "outputs": [],
      "source": [
        "# for cell_id, testing_data in test_data.items():\n",
        "#     print(testing_data)\n",
        "\n",
        "\n",
        "# for idx, data in test_data.values():\n",
        "#     frames = [idx, data]\n",
        "#     result = pd.concat(frames)\n",
        "\n",
        "print(type(training_data))\n",
        "#training_data\n",
        "#cell_id_training_data_map = {k: v for k, v in result.groupby(\"cell_id\")}\n",
        "#print(cell_id_training_data_map[1])\n",
        "\n",
        "# for cell_id, testing_data in cell_id_training_data_map.items():\n",
        "#     testing_data\n",
        "#     pass\n",
        "    \n",
        "# test_data = {key: val[3000:] for key, val in training_data.items()}\n",
        "training_data[1]\n",
        "# for idx, data in test_data.items():\n",
        "#     print(data)\n",
        "#     data.sort_values(by=['loc_x', 'loc_y'],  inplace=True)\n",
        "#     print(data)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "customInput": null,
        "customOutput": null,
        "executionStartTime": 1662943446815,
        "executionStopTime": 1662943446993,
        "originalKey": "f3767341-be2e-4f26-a63c-5bd26d224367",
        "requestMsgId": "f3767341-be2e-4f26-a63c-5bd26d224367",
        "showInput": true
      },
      "outputs": [],
      "source": [
        "# dictionary[new_key] = dictionary[old_key]\n",
        "# del dictionary[old_key]\n",
        "\n",
        "#test_data[1] = test_data[2]\n",
        "#del training_data[2]\n",
        "\n",
        "# def append_value(dict_obj, key, value):\n",
        "#     # Check if key exist in dict or not\n",
        "#     if key in dict_obj:\n",
        "#         # Key exist in dict.\n",
        "#         # Check if type of value of key is list or not\n",
        "#         if not isinstance(dict_obj[key], list):\n",
        "#             # If type is not list then make it list\n",
        "#             dict_obj[key] = [dict_obj[key]]\n",
        "#         # Append the value in list\n",
        "#         dict_obj[key].append(value)\n",
        "#     else:\n",
        "#         # As key is not in dict,\n",
        "#         # so, add key-value pair\n",
        "#         dict_obj[key] = value\n",
        "\n",
        "# test_data_one= []\n",
        "# test_data_one.append(test_data[1])\n",
        "\n",
        "# test_data_two = []\n",
        "# test_data_two.append(test_data[2])\n",
        "\n",
        "\n",
        "\n",
        "#test_data_final[1] = test_data[1]\n",
        "#test_data_final[1].append(test_data[2])\n",
        "\n",
        "#test_data_two[0]\n",
        "# joined_list = [*test_data_one, *test_data_two]\n",
        "# joined_list\n",
        "# cell_id_training_data_map = {k: v for k, v in test_data_final.groupby(\"cell_id\")}\n",
        "# cell_id_training_data_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "code_folding": [],
        "collapsed": false,
        "customInput": null,
        "customOutput": null,
        "executionStartTime": 1664303075843,
        "executionStopTime": 1664303096582,
        "hidden_ranges": [],
        "originalKey": "124688ea-c98e-4363-93a2-23c74cc8df99",
        "requestMsgId": "124688ea-c98e-4363-93a2-23c74cc8df99",
        "showInput": true
      },
      "outputs": [],
      "source": [
        "p_train = 20\n",
        "p_test = 100\n",
        "maxiter = 25\n",
        "load_model = False\n",
        "save_model = False\n",
        "bdt_model_path = \"/tmp/incremental\"\n",
        "bdt_model_name = \"/bdt.mod\"\n",
        "\n",
        "bayesian_digital_twin, test_data, loss_vs_iter = bdt(\n",
        "    p_train=p_train,\n",
        "    p_test=p_test,\n",
        "    maxiter=maxiter,\n",
        "    n_sim=n_sim,\n",
        "    desired_idxs_train=[1],\n",
        "    desired_idxs_test=[1],\n",
        "    desired_idxs_train_update=[1],\n",
        "    desired_idxs_test_update=[1],\n",
        "    load_model=load_model,\n",
        "    save_model=save_model,\n",
        "    model_path=bdt_model_path,\n",
        "    model_name=bdt_model_name,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "customInput": null,
        "customOutput": null,
        "executionStopTime": 1661906392660,
        "originalKey": "e0b47602-4620-4a05-b6a8-2ad0f4115edd",
        "requestMsgId": "e0b47602-4620-4a05-b6a8-2ad0f4115edd",
        "showInput": true
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "bento_stylesheets": {
      "bento/extensions/flow/main.css": true,
      "bento/extensions/kernel_selector/main.css": true,
      "bento/extensions/kernel_ui/main.css": true,
      "bento/extensions/new_kernel/main.css": true,
      "bento/extensions/system_usage/main.css": true,
      "bento/extensions/theme/main.css": true
    },
    "captumWidgetMessage": {},
    "dataExplorerConfig": {},
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "last_base_url": "https://devvm6826.prn0.facebook.com:8090/",
    "last_kernel_id": "14579b68-b9a0-4ca5-acc9-3516d6d93314",
    "last_msg_id": "6ac66a4a-1212acb71722493a782edcef_199",
    "last_server_session_id": "a7f2ca9e-552f-4589-9abc-85eb2f7cb4d6",
    "outputWidgetContext": {}
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
